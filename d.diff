diff --git a/python/src/.DS_Store b/python/src/.DS_Store
new file mode 100644
index 0000000..14a15a2
Binary files /dev/null and b/python/src/.DS_Store differ
diff --git a/python/src/environment/physical.py b/python/src/environment/physical.py
index 1353009..ed7c492 100644
--- a/python/src/environment/physical.py
+++ b/python/src/environment/physical.py
@@ -74,8 +74,8 @@ class InvertedPendulumContinuousControlPhysical(gym.Env):
 
     def step(self, action):
         action = np.clip(action, -1.0, 1.0)[0]
-        self.client.move(action.item())
-        time.sleep(0.001)
+        res = self.client.move(action.item())
+        time.sleep(0.01)
         response = self.client.sense()
         if response is None:
             return self.last_step_return
@@ -92,7 +92,7 @@ class InvertedPendulumContinuousControlPhysical(gym.Env):
         terminated = limitL or limitR
 
         # Calculate reward
-        reward = self._calculate_reward(obs, terminated)
+        reward = self.inverted_pendulum_reward(*obs, action)
 
         # Check truncation
         self.t += 1
@@ -103,22 +103,67 @@ class InvertedPendulumContinuousControlPhysical(gym.Env):
 
         return obs, reward, terminated, truncated, {}
 
-    def _calculate_reward(self, obs, terminated) -> float:
-        #np.array([x, x_dot, np.cos(theta), np.sin(theta), theta_dot])
-        
-        x, x_dot, cos_theta, sin_theta, theta_dot = obs
-        theta = self.state[2]
-        
-        reward_theta = (cos_theta + 1.0) / 2.0
-        reward_x = np.cos(x * (np.pi / 2.0))
-        reward_bonus = 0.0
-        if np.cos(theta) > 0.999:
-            reward_bonus = 0.5
-        reward = reward_theta * reward_x + reward_bonus
-        if terminated:
-            reward = -1.0
-            
+    def inverted_pendulum_reward(
+        self,
+        x: float,
+        x_dot: float,
+        cos_theta: float,
+        sin_theta: float,
+        theta_dot: float,
+        action: float,
+        w_x: float = 0.05,
+        w_x_dot: float = 0.005,
+        w_theta: float = 1.0,
+        w_theta_dot: float = 0.01,
+        w_u: float = 0.001
+    ) -> float:
+        """
+        Computes the reward for the inverted pendulum swing-up task.
+
+        Args:
+            x (float): Position of the cart.
+            x_dot (float): Velocity of the cart.
+            cos_theta (float): cos(theta) for the pendulum angle.
+            sin_theta (float): sin(theta) for the pendulum angle (optionally used if needed).
+            theta_dot (float): Angular velocity of the pendulum.
+            action (float): Control action (torque) applied.
+            w_x (float): Weight for cart position penalty.
+            w_x_dot (float): Weight for cart velocity penalty.
+            w_theta (float): Weight for the pendulum angle penalty.
+            w_theta_dot (float): Weight for the pendulum angular velocity penalty.
+            w_u (float): Weight for action (control effort) penalty.
+
+        Returns:
+            float: The scalar reward.
+        """
+
+        # 1. Measure deviation from upright: (1 - cos(theta))^2
+        #    cos_theta is close to 1 when the pendulum is upright (theta ~= 0).
+        #    We square it so that being far away from cos_theta=1 is penalized more.
+        theta_error = (1.0 - cos_theta) ** 2
+
+        # 2. Cart position and velocity penalties
+        cart_position_error = x ** 2
+        cart_velocity_error = x_dot ** 2
+
+        # 3. Pendulum spinning penalty
+        pendulum_angular_velocity_error = theta_dot ** 2
+
+        # 4. Action penalty (to encourage smooth, minimal control effort)
+        action_penalty = action ** 2
+
+        # 5. Combine all penalty terms (weighted) and subtract from a base reward (e.g. 1)
+        penalty = (
+            w_x * cart_position_error
+            + w_x_dot * cart_velocity_error
+            + w_theta * theta_error
+            + w_theta_dot * pendulum_angular_velocity_error
+            + w_u * action_penalty
+        )
+        reward = 1.0 - penalty
+
         return reward
 
+
     def __del__(self) -> None:
         self.client.close()
diff --git a/python/src/environment/sim.py b/python/src/environment/sim.py
index 7ec463a..519def2 100644
--- a/python/src/environment/sim.py
+++ b/python/src/environment/sim.py
@@ -99,18 +99,64 @@ class InvertedPendulumContinuousControlSim(Env):
         self.t += 1
 
         # Reward calculation
-        reward_theta = (np.cos(theta) + 1.0) / 2.0
-        reward_x = np.cos((x / self.x_threshold) * (np.pi / 2.0))
-        reward_bonus = 0.0
-        if np.cos(theta) > 0.999:
-            reward_bonus = 0.5
-        reward = reward_theta * reward_x + reward_bonus
-        if terminated:
-            reward = -1.0
+        reward = self.inverted_pendulum_reward(x, x_dot, np.cos(theta), np.sin(theta), theta_dot, action)
 
         obs = np.array([x, x_dot, np.cos(theta), np.sin(theta), theta_dot])
 
         return obs, reward, terminated, truncated, {}
+    
+    def inverted_pendulum_reward(
+        self,
+        x: float,
+        x_dot: float,
+        cos_theta: float,
+        sin_theta: float,
+        theta_dot: float,
+        action: float,
+        w_x: float = 0.1,          # Increased from 0.05 to discourage large movements
+        w_x_dot: float = 0.1,      # Increased from 0.005 to penalize fast cart motion
+        w_theta: float = 1.0,      # Main penalty for being away from upright
+        w_theta_dot: float = 0.2,  # Increased from 0.01 to prevent spinning
+        w_u: float = 0.01          # Increased from 0.001 to discourage aggressive actions
+    ) -> float:
+        """
+        Computes the reward for the inverted pendulum swing-up task.
+        Updated to penalize excessive spinning and jerky motions.
+        """
+        
+        # 1. Upright stability reward (cos(theta) ~= 1 when upright)
+        theta_error = (1.0 - cos_theta) ** 2
+        
+        # 2. Cart stability penalties
+        cart_position_penalty = x ** 2
+        cart_velocity_penalty = x_dot ** 2
+        
+        # 3. Angular velocity penalty (prevents spinning)
+        angular_velocity_penalty = theta_dot ** 2
+        
+        # 4. Action smoothness penalty
+        action_penalty = action ** 2
+        
+        # 5. Additional penalty for excessive spinning
+        spin_penalty = 0.0
+        if abs(theta_dot) > 8.0:  # Radians per second threshold
+            spin_penalty = 2.0 * (abs(theta_dot) - 8.0)  # Linear penalty beyond threshold
+
+        # Combine all components
+        total_penalty = (
+            w_x * cart_position_penalty +
+            w_x_dot * cart_velocity_penalty +
+            w_theta * theta_error +
+            w_theta_dot * angular_velocity_penalty +
+            w_u * action_penalty +
+            spin_penalty
+        )
+        
+        # Base reward encourages maintaining upright position
+        reward = 1.0 - total_penalty
+
+        # Clip reward to reasonable range
+        return max(min(reward, 1.0), -2.0)
 
     def reset(self, seed=None, options=None):
         super().reset(seed=seed)
diff --git a/python/src/gui.py b/python/src/gui.py
index d05c7bc..0d236cf 100644
--- a/python/src/gui.py
+++ b/python/src/gui.py
@@ -456,7 +456,7 @@ class PendulumVisualizerDPG:
         extent = self.state.get("extent", 1000)
         current_position = self.state["current_position"]
         if extent != 0:
-            pendulum_offset_from_center = (current_position / extent) * (
+            pendulum_offset_from_center = (current_position) * (
                 (max_x_pos - min_x_pos - 90) / 2
             )
         else:
@@ -574,10 +574,10 @@ class PendulumVisualizerDPG:
         number_of_points = 1000
 
         dpg.set_axis_limits(
-            "current_position_y_axis", -self.state["extent"], self.state["extent"]
+            "current_position_y_axis", -1, 1
         )
         dpg.set_axis_limits(
-            "velocity_y_axis", -self.state["extent"], self.state["extent"]
+            "velocity_y_axis", -1, 1
         )
 
         # Update angle chart data
diff --git a/python/src/serial_communication/client.py b/python/src/serial_communication/client.py
index e5218bd..b7ad7a4 100644
--- a/python/src/serial_communication/client.py
+++ b/python/src/serial_communication/client.py
@@ -80,9 +80,9 @@ class SerialCommunicator:
         """Send a sense command and wait for the response."""
         return self._send_command("sense", timeout=timeout)
 
-    def move(self, distance: int, timeout: float = 0.1) -> Dict[str, Any]:
+    def move(self, speed: float, timeout: float = 0.1) -> Dict[str, Any]:
         """Send a move command with the specified distance and wait for the response."""
-        return self._send_command("move", {"distance": distance}, timeout=timeout)
+        return self._send_command("move", {"speed": speed}, timeout=timeout)
 
     def reset(self, timeout: float = 0.1) -> Dict[str, Any]:
         """Send a reset command and wait for the response."""
